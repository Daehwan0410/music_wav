{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c957ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "# Librosa (the mother of audio files)\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d881d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375d3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossings_mean=[]\n",
    "zero_crossings_var = []\n",
    "y_perc_mean=[] \n",
    "y_perc_var=[]\n",
    "y_harm_mean=[] \n",
    "y_harm_var=[]\n",
    "spectral_centroids_mean=[] \n",
    "spectral_centroids_var=[]\n",
    "spectral_rolloff_mean=[]\n",
    "spectral_rolloff_var=[]\n",
    "chromagram_mean=[]\n",
    "chromagram_var=[]\n",
    "mfcc_mean=[]\n",
    "mfcc_var=[]\n",
    "labels= []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d476050",
   "metadata": {},
   "source": [
    "## 음악데이터 특정 컬럼 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8506e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioFeatures(music):\n",
    "    # Zero-crdssing\n",
    "    wav, sr = librosa.load(f'./음악/{music}')\n",
    "#     wav, sr = librosa.load(f'./Data/genres_original/classical/classical.00000.wav')\n",
    "    \n",
    "    zero_crossings = librosa.zero_crossings(wav, pad=False)\n",
    "    zero_crossings_mean.append(zero_crossings.mean())\n",
    "    zero_crossings_var.append(zero_crossings.var())\n",
    "\n",
    "    #Harmonics and Perceptrual\n",
    "    y_harm, y_perc = librosa.effects.hpss(wav)\n",
    "    y_harm_mean.append(y_harm.mean())\n",
    "    y_harm_var.append(y_harm.var())\n",
    "    y_perc_mean.append(y_perc.mean())\n",
    "    y_perc_var.append(y_perc.var())\n",
    "\n",
    "\n",
    "    #Spectral Centroid / Rolloff\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(wav, sr=sr)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(wav, sr=sr)[0]\n",
    "\n",
    "    spectral_centroids_mean.append(spectral_centroids.mean())\n",
    "    spectral_centroids_var.append(spectral_centroids.var())\n",
    "    spectral_rolloff_mean.append(spectral_rolloff.mean())\n",
    "    spectral_rolloff_var.append(spectral_rolloff.var())\n",
    "\n",
    "    #Mel-Frequency Cepstral Coefficients\n",
    "    mfccs = librosa.feature.mfcc(wav, sr=sr)\n",
    "    mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\n",
    "    mfcc_mean.append(mfccs.mean())\n",
    "    mfcc_var.append(mfccs.var())\n",
    "\n",
    "    #Chroma Frequencies\n",
    "    hop_length = 5000\n",
    "    chromagram = librosa.feature.chroma_stft(wav, sr=sr, hop_length=hop_length)\n",
    "    chromagram_mean.append(chromagram.mean())\n",
    "    chromagram_var.append(chromagram.var())\n",
    "\n",
    "    #Label\n",
    "    labels.append(music.split('.')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a98217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "musics = list(os.listdir('./음악/'))\n",
    "for i in musics:\n",
    "    AudioFeatures(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = {\"zero_crossings_mean\": zero_crossings_mean,\"zero_crossings_var\": zero_crossings_var ,\"y_perc_mean\": y_perc_mean, \n",
    "         \"y_perc_var\": y_perc_var, \"y_harm_mean\":y_harm_mean,\"y_harm_var\" : y_harm_var,\"spectral_centroids_mean\" : spectral_centroids_mean,\n",
    "         \"spectral_centroids_var\" : spectral_centroids_var,\"spectral_rolloff_mean\" :spectral_rolloff_mean,\"spectral_rolloff_var\" : spectral_rolloff_var,\n",
    "         \"chromagram_mean\" :chromagram_mean, \"chromagram_var\" :chromagram_var ,\"mfcc_mean\" :mfcc_mean, \"mfcc_var\" : mfcc_var, \"labels\" : labels \n",
    "        }\n",
    "df = pd.DataFrame(music_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d5339a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blues      100\n",
       "country    100\n",
       "disco      100\n",
       "reggae     100\n",
       "rock       100\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "785cefaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blues': 0, 'country': 1, 'disco': 2, 'reggae': 3, 'rock': 4}\n"
     ]
    }
   ],
   "source": [
    "label_index = dict()\n",
    "index_label = dict()\n",
    "for i, x in enumerate(df.labels.unique()):\n",
    "    label_index[x] = i\n",
    "    index_label[i] = x\n",
    "print(label_index)\n",
    "df.labels = [label_index[l] for l in df.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c55972f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['labels']\n",
    "X = df.loc[:, df.columns != 'labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c60535",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a77936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = min_max_scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18c28c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assess(model,title = \"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print('Accuracy', title, ':', round(accuracy_score(y_test, preds), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5aab1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "790c54cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naive Bayes : 0.84 \n",
      "\n",
      "Accuracy Stochastic Gradient Descent : 0.848 \n",
      "\n",
      "Accuracy KNN : 0.848 \n",
      "\n",
      "Accuracy Decission trees : 1.0 \n",
      "\n",
      "Accuracy Random Forest : 0.984 \n",
      "\n",
      "Accuracy Support Vector Machine : 0.92 \n",
      "\n",
      "Accuracy Logistic Regression : 0.832 \n",
      "\n",
      "Accuracy Neural Nets : 0.728 \n",
      "\n",
      "[13:59:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy Cross Gradient Booster : 1.0 \n",
      "\n",
      "[13:59:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy Cross Gradient Booster (Random Forest) : 1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "model_assess(nb, \"Naive Bayes\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "sgd = SGDClassifier(max_iter=5000, random_state=0)\n",
    "model_assess(sgd, \"Stochastic Gradient Descent\")\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "model_assess(knn, \"KNN\")\n",
    "\n",
    "# Decission trees\n",
    "tree = DecisionTreeClassifier()\n",
    "model_assess(tree, \"Decission trees\")\n",
    "\n",
    "# Random Forest\n",
    "rforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\n",
    "model_assess(rforest, \"Random Forest\")\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "model_assess(svm, \"Support Vector Machine\")\n",
    "\n",
    "# Logistic Regression\n",
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "model_assess(lg, \"Logistic Regression\")\n",
    "\n",
    "# Neural Nets\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\n",
    "model_assess(nn, \"Neural Nets\")\n",
    "\n",
    "# Cross Gradient Booster\n",
    "xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "model_assess(xgb, \"Cross Gradient Booster\")\n",
    "\n",
    "# Cross Gradient Booster (Random Forest)\n",
    "xgbrf = XGBRFClassifier(objective= 'multi:softmax')\n",
    "model_assess(xgbrf, \"Cross Gradient Booster (Random Forest)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535d0d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rforest = RandomForestClassifier()\n",
    "search_params = {\n",
    "    'n_estimators'      : [100, 500,1000],\n",
    "#     'criterion'         : ['gini', 'entropy'],\n",
    "    'max_features'      : [\"auto\", 3,5,10, 20],\n",
    "    'random_state'      : [2525,0],\n",
    "#     'n_jobs'            : [1],\n",
    "    'min_samples_split' : [3, 10, 20, 50, 100,150,200,300],\n",
    "    'max_depth'         : [3, 10, 20, 50, 100,150,200,300]\n",
    "#     'bootstrap'         : [False],#True\n",
    "#     'oob_score'         : [False],#True\n",
    "}\n",
    "gs = GridSearchCV(RandomForestClassifier(),search_params, cv=5, verbose=True)\n",
    "gs.fit(X_train, y_train)\n",
    "preds = gs.predict(TestScaled)\n",
    "print('Accuracy :', round(accuracy_score(y_test, preds), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,rforest.predict(TestScaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ce961a64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0393\n",
       "                \n",
       "                    &plusmn; 0.0175\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                spectral_rolloff_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 81.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0364\n",
       "                \n",
       "                    &plusmn; 0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                chromagram_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.72%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0243\n",
       "                \n",
       "                    &plusmn; 0.0070\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                chromagram_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0229\n",
       "                \n",
       "                    &plusmn; 0.0073\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                y_harm_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.53%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0200\n",
       "                \n",
       "                    &plusmn; 0.0140\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                y_perc_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0107\n",
       "                \n",
       "                    &plusmn; 0.0111\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                y_harm_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0079\n",
       "                \n",
       "                    &plusmn; 0.0029\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                mfcc_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.94%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0071\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                spectral_rolloff_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.37%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0064\n",
       "                \n",
       "                    &plusmn; 0.0029\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                y_perc_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0057\n",
       "                \n",
       "                    &plusmn; 0.0035\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                mfcc_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0057\n",
       "                \n",
       "                    &plusmn; 0.0035\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                spectral_centroids_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0043\n",
       "                \n",
       "                    &plusmn; 0.0053\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                spectral_centroids_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.27%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0036\n",
       "                \n",
       "                    &plusmn; 0.0045\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                zero_crossings_mean\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0029\n",
       "                \n",
       "                    &plusmn; 0.0029\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                zero_crossings_var\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(estimator=rforest, random_state=1)\n",
    "perm.fit(XScaled, y_train)\n",
    "\n",
    "eli5.show_weights(estimator=perm, feature_names  = XScaled.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6941c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa= pd.read_csv(\"./Data/features_30_sec.csv\")\n",
    "y = aa.label\n",
    "aa.drop([\"filename\",\"label\"], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10c9543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blues': 0, 'country': 1, 'disco': 2, 'reggae': 3, 'rock': 4}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57dd6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 1024)              16384     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 691,205\n",
      "Trainable params: 691,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024,input_shape=(X_train.shape[1],),activation='relu', kernel_regularizer = keras.regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer = keras.regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer = keras.regularizers.l2(0.003)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer = keras.regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "022b3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=1e-4)\n",
    "model.compile(optimizer=adam,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8b50b71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.9842 - accuracy: 0.7040 - val_loss: 1.1871 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18714, saving model to clf-resnet-checkpoint.hdf5\n",
      "Epoch 2/600\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.9592 - accuracy: 0.6960 - val_loss: 1.1905 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.18714\n",
      "Epoch 3/600\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.9867 - accuracy: 0.6907 - val_loss: 1.1789 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18714 to 1.17892, saving model to clf-resnet-checkpoint.hdf5\n",
      "Epoch 4/600\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.9719 - accuracy: 0.6613 - val_loss: 1.2052 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17892\n",
      "Epoch 5/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9855 - accuracy: 0.6853 - val_loss: 1.1692 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.17892 to 1.16919, saving model to clf-resnet-checkpoint.hdf5\n",
      "Epoch 6/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.9535 - accuracy: 0.7120 - val_loss: 1.1900 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.16919\n",
      "Epoch 7/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.9587 - accuracy: 0.7013 - val_loss: 1.1804 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.16919\n",
      "Epoch 8/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.9660 - accuracy: 0.7040 - val_loss: 1.1690 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.16919 to 1.16897, saving model to clf-resnet-checkpoint.hdf5\n",
      "Epoch 9/600\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.9746 - accuracy: 0.6987 - val_loss: 1.1832 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.16897\n",
      "Epoch 10/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.9831 - accuracy: 0.6933 - val_loss: 1.1704 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16897\n",
      "Epoch 11/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9355 - accuracy: 0.7093 - val_loss: 1.1897 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.16897\n",
      "Epoch 12/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9701 - accuracy: 0.7253 - val_loss: 1.1771 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.16897\n",
      "Epoch 13/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9708 - accuracy: 0.7173 - val_loss: 1.1677 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.16897 to 1.16773, saving model to clf-resnet-checkpoint.hdf5\n",
      "Epoch 14/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.9931 - accuracy: 0.6933 - val_loss: 1.1837 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.16773\n",
      "Epoch 15/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9701 - accuracy: 0.6880 - val_loss: 1.1754 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.16773\n",
      "Epoch 16/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9408 - accuracy: 0.7227 - val_loss: 1.1687 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.16773\n",
      "Epoch 17/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9356 - accuracy: 0.7360 - val_loss: 1.1702 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.16773\n",
      "Epoch 18/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9985 - accuracy: 0.7013 - val_loss: 1.1772 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.16773\n",
      "Epoch 19/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9771 - accuracy: 0.7120 - val_loss: 1.1584 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.16773 to 1.15836, saving model to clf-resnet-checkpoint.hdf5\n",
      "Epoch 20/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9531 - accuracy: 0.7120 - val_loss: 1.1855 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.15836\n",
      "Epoch 21/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9700 - accuracy: 0.6960 - val_loss: 1.1731 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.15836\n",
      "Epoch 22/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 1.0009 - accuracy: 0.6773 - val_loss: 1.2035 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.15836\n",
      "Epoch 23/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9572 - accuracy: 0.7173 - val_loss: 1.1718 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.15836\n",
      "Epoch 24/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9480 - accuracy: 0.7253 - val_loss: 1.1971 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.15836\n",
      "Epoch 25/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9570 - accuracy: 0.7147 - val_loss: 1.1755 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.15836\n",
      "Epoch 26/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9681 - accuracy: 0.7093 - val_loss: 1.1880 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.15836\n",
      "Epoch 27/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9340 - accuracy: 0.7360 - val_loss: 1.1772 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.15836\n",
      "Epoch 28/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9354 - accuracy: 0.7253 - val_loss: 1.1808 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.15836\n",
      "Epoch 29/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9491 - accuracy: 0.7040 - val_loss: 1.1859 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.15836\n",
      "Epoch 30/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9186 - accuracy: 0.7227 - val_loss: 1.1656 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.15836\n",
      "Epoch 31/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9423 - accuracy: 0.7120 - val_loss: 1.1959 - val_accuracy: 0.6960\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.15836\n",
      "Epoch 32/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9659 - accuracy: 0.6853 - val_loss: 1.1750 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.15836\n",
      "Epoch 33/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9630 - accuracy: 0.6907 - val_loss: 1.1792 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.15836\n",
      "Epoch 34/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9618 - accuracy: 0.7013 - val_loss: 1.1764 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.15836\n",
      "Epoch 35/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9542 - accuracy: 0.7200 - val_loss: 1.1757 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.15836\n",
      "Epoch 36/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9446 - accuracy: 0.7173 - val_loss: 1.1946 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.15836\n",
      "Epoch 37/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9468 - accuracy: 0.7440 - val_loss: 1.1724 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.15836\n",
      "Epoch 38/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9679 - accuracy: 0.6960 - val_loss: 1.1719 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.15836\n",
      "Epoch 39/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9418 - accuracy: 0.7360 - val_loss: 1.1871 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.15836\n",
      "Epoch 40/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9487 - accuracy: 0.7253 - val_loss: 1.1674 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.15836\n",
      "Epoch 41/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9806 - accuracy: 0.6987 - val_loss: 1.1789 - val_accuracy: 0.6320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_loss did not improve from 1.15836\n",
      "Epoch 42/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9529 - accuracy: 0.6880 - val_loss: 1.1739 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.15836\n",
      "Epoch 43/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9978 - accuracy: 0.7120 - val_loss: 1.1751 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.15836\n",
      "Epoch 44/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9305 - accuracy: 0.7200 - val_loss: 1.1741 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.15836\n",
      "Epoch 45/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.9519 - accuracy: 0.7253 - val_loss: 1.1805 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.15836\n",
      "Epoch 46/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9845 - accuracy: 0.7120 - val_loss: 1.1807 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.15836\n",
      "Epoch 47/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9738 - accuracy: 0.7120 - val_loss: 1.1975 - val_accuracy: 0.6880\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.15836\n",
      "Epoch 48/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9682 - accuracy: 0.6960 - val_loss: 1.1808 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.15836\n",
      "Epoch 49/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9508 - accuracy: 0.7120 - val_loss: 1.1702 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.15836\n",
      "Epoch 50/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9559 - accuracy: 0.7120 - val_loss: 1.1808 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.15836\n",
      "Epoch 51/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9730 - accuracy: 0.6933 - val_loss: 1.1855 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.15836\n",
      "Epoch 52/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9562 - accuracy: 0.7253 - val_loss: 1.1902 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.15836\n",
      "Epoch 53/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9518 - accuracy: 0.7253 - val_loss: 1.1667 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.15836\n",
      "Epoch 54/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9653 - accuracy: 0.7227 - val_loss: 1.2043 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.15836\n",
      "Epoch 55/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9539 - accuracy: 0.7120 - val_loss: 1.1652 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.15836\n",
      "Epoch 56/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9464 - accuracy: 0.7147 - val_loss: 1.1799 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.15836\n",
      "Epoch 57/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9517 - accuracy: 0.7040 - val_loss: 1.1735 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.15836\n",
      "Epoch 58/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9633 - accuracy: 0.7200 - val_loss: 1.1768 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.15836\n",
      "Epoch 59/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9324 - accuracy: 0.7200 - val_loss: 1.1809 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.15836\n",
      "Epoch 60/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9481 - accuracy: 0.7067 - val_loss: 1.1742 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.15836\n",
      "Epoch 61/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9403 - accuracy: 0.7200 - val_loss: 1.1841 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.15836\n",
      "Epoch 62/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9404 - accuracy: 0.7253 - val_loss: 1.1923 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.15836\n",
      "Epoch 63/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9628 - accuracy: 0.7360 - val_loss: 1.1687 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.15836\n",
      "Epoch 64/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9378 - accuracy: 0.7253 - val_loss: 1.1820 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.15836\n",
      "Epoch 65/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9367 - accuracy: 0.6987 - val_loss: 1.1779 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.15836\n",
      "Epoch 66/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9607 - accuracy: 0.7120 - val_loss: 1.1768 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.15836\n",
      "Epoch 67/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9317 - accuracy: 0.7093 - val_loss: 1.1792 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.15836\n",
      "Epoch 68/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9528 - accuracy: 0.7227 - val_loss: 1.1695 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.15836\n",
      "Epoch 69/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9284 - accuracy: 0.7173 - val_loss: 1.1794 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.15836\n",
      "Epoch 70/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9339 - accuracy: 0.7093 - val_loss: 1.1598 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.15836\n",
      "Epoch 71/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.9281 - accuracy: 0.7387 - val_loss: 1.1919 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.15836\n",
      "Epoch 72/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9256 - accuracy: 0.7493 - val_loss: 1.1779 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.15836\n",
      "Epoch 73/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9452 - accuracy: 0.7093 - val_loss: 1.1764 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.15836\n",
      "Epoch 74/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9351 - accuracy: 0.7307 - val_loss: 1.1774 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.15836\n",
      "Epoch 75/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9405 - accuracy: 0.7200 - val_loss: 1.1889 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.15836\n",
      "Epoch 76/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9430 - accuracy: 0.6853 - val_loss: 1.1727 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.15836\n",
      "Epoch 77/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9680 - accuracy: 0.7147 - val_loss: 1.1795 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.15836\n",
      "Epoch 78/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9493 - accuracy: 0.7227 - val_loss: 1.1692 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.15836\n",
      "Epoch 79/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9226 - accuracy: 0.7387 - val_loss: 1.1599 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.15836\n",
      "Epoch 80/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9282 - accuracy: 0.7147 - val_loss: 1.1724 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.15836\n",
      "Epoch 81/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9139 - accuracy: 0.7253 - val_loss: 1.1700 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.15836\n",
      "Epoch 82/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9119 - accuracy: 0.7467 - val_loss: 1.1806 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.15836\n",
      "Epoch 83/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9604 - accuracy: 0.6853 - val_loss: 1.1622 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.15836\n",
      "Epoch 84/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9247 - accuracy: 0.7227 - val_loss: 1.1751 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.15836\n",
      "Epoch 85/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9332 - accuracy: 0.7253 - val_loss: 1.1626 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.15836\n",
      "Epoch 86/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9432 - accuracy: 0.7200 - val_loss: 1.1857 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.15836\n",
      "Epoch 87/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.9491 - accuracy: 0.7120 - val_loss: 1.1714 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.15836\n",
      "Epoch 88/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.9099 - accuracy: 0.7413 - val_loss: 1.1909 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.15836\n",
      "Epoch 89/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8993 - accuracy: 0.7413 - val_loss: 1.1762 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.15836\n",
      "Epoch 90/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9342 - accuracy: 0.7227 - val_loss: 1.1749 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.15836\n",
      "Epoch 91/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.9196 - accuracy: 0.7360 - val_loss: 1.1765 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.15836\n",
      "Epoch 92/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9587 - accuracy: 0.7067 - val_loss: 1.1722 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.15836\n",
      "Epoch 93/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9413 - accuracy: 0.7280 - val_loss: 1.1697 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.15836\n",
      "Epoch 94/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9254 - accuracy: 0.7147 - val_loss: 1.1776 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.15836\n",
      "Epoch 95/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8975 - accuracy: 0.7147 - val_loss: 1.1877 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.15836\n",
      "Epoch 96/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9342 - accuracy: 0.7200 - val_loss: 1.1969 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.15836\n",
      "Epoch 97/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9290 - accuracy: 0.7227 - val_loss: 1.1955 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.15836\n",
      "Epoch 98/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9020 - accuracy: 0.7147 - val_loss: 1.2017 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.15836\n",
      "Epoch 99/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9204 - accuracy: 0.7307 - val_loss: 1.1857 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.15836\n",
      "Epoch 100/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9341 - accuracy: 0.7173 - val_loss: 1.1931 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.15836\n",
      "Epoch 101/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9155 - accuracy: 0.7333 - val_loss: 1.1806 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.15836\n",
      "Epoch 102/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9408 - accuracy: 0.7200 - val_loss: 1.1903 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.15836\n",
      "Epoch 103/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9204 - accuracy: 0.7360 - val_loss: 1.2068 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.15836\n",
      "Epoch 104/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9280 - accuracy: 0.7360 - val_loss: 1.1900 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.15836\n",
      "Epoch 105/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8898 - accuracy: 0.7413 - val_loss: 1.1963 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.15836\n",
      "Epoch 106/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9310 - accuracy: 0.7200 - val_loss: 1.1757 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.15836\n",
      "Epoch 107/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9604 - accuracy: 0.6933 - val_loss: 1.1726 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.15836\n",
      "Epoch 108/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9000 - accuracy: 0.7360 - val_loss: 1.1687 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.15836\n",
      "Epoch 109/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9109 - accuracy: 0.7413 - val_loss: 1.1823 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.15836\n",
      "Epoch 110/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9275 - accuracy: 0.7120 - val_loss: 1.1820 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.15836\n",
      "Epoch 111/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9069 - accuracy: 0.7520 - val_loss: 1.1863 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.15836\n",
      "Epoch 112/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9263 - accuracy: 0.7360 - val_loss: 1.1733 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.15836\n",
      "Epoch 113/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8946 - accuracy: 0.7307 - val_loss: 1.1772 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.15836\n",
      "Epoch 114/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.9167 - accuracy: 0.7173 - val_loss: 1.1778 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.15836\n",
      "Epoch 115/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9138 - accuracy: 0.7253 - val_loss: 1.2075 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.15836\n",
      "Epoch 116/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8885 - accuracy: 0.7440 - val_loss: 1.1734 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.15836\n",
      "Epoch 117/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9217 - accuracy: 0.7173 - val_loss: 1.1886 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.15836\n",
      "Epoch 118/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9209 - accuracy: 0.7493 - val_loss: 1.1834 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.15836\n",
      "Epoch 119/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9331 - accuracy: 0.7067 - val_loss: 1.1717 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.15836\n",
      "Epoch 120/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9193 - accuracy: 0.7307 - val_loss: 1.1891 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.15836\n",
      "Epoch 121/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9287 - accuracy: 0.7173 - val_loss: 1.1752 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.15836\n",
      "Epoch 122/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9288 - accuracy: 0.7280 - val_loss: 1.1680 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.15836\n",
      "Epoch 123/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9287 - accuracy: 0.6987 - val_loss: 1.1739 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.15836\n",
      "Epoch 124/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9051 - accuracy: 0.7200 - val_loss: 1.1802 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.15836\n",
      "Epoch 125/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9182 - accuracy: 0.7280 - val_loss: 1.1667 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.15836\n",
      "Epoch 126/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8955 - accuracy: 0.7333 - val_loss: 1.1802 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.15836\n",
      "Epoch 127/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9051 - accuracy: 0.7200 - val_loss: 1.1823 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.15836\n",
      "Epoch 128/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9191 - accuracy: 0.7413 - val_loss: 1.1827 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.15836\n",
      "Epoch 129/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9009 - accuracy: 0.7200 - val_loss: 1.1723 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.15836\n",
      "Epoch 130/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9157 - accuracy: 0.7387 - val_loss: 1.1866 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.15836\n",
      "Epoch 131/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9085 - accuracy: 0.7173 - val_loss: 1.1851 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.15836\n",
      "Epoch 132/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9110 - accuracy: 0.7253 - val_loss: 1.1941 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.15836\n",
      "Epoch 133/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9067 - accuracy: 0.7467 - val_loss: 1.1837 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.15836\n",
      "Epoch 134/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9061 - accuracy: 0.7413 - val_loss: 1.1890 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.15836\n",
      "Epoch 135/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8933 - accuracy: 0.7227 - val_loss: 1.1833 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.15836\n",
      "Epoch 136/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9248 - accuracy: 0.7173 - val_loss: 1.1803 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.15836\n",
      "Epoch 137/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8915 - accuracy: 0.7093 - val_loss: 1.1952 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.15836\n",
      "Epoch 138/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8986 - accuracy: 0.7387 - val_loss: 1.1782 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.15836\n",
      "Epoch 139/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9025 - accuracy: 0.7493 - val_loss: 1.1886 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.15836\n",
      "Epoch 140/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9124 - accuracy: 0.7173 - val_loss: 1.1833 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.15836\n",
      "Epoch 141/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8968 - accuracy: 0.7360 - val_loss: 1.1848 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.15836\n",
      "Epoch 142/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8773 - accuracy: 0.7467 - val_loss: 1.1768 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.15836\n",
      "Epoch 143/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8500 - accuracy: 0.7520 - val_loss: 1.1969 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.15836\n",
      "Epoch 144/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8989 - accuracy: 0.7360 - val_loss: 1.1783 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.15836\n",
      "Epoch 145/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8884 - accuracy: 0.7253 - val_loss: 1.1842 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.15836\n",
      "Epoch 146/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9213 - accuracy: 0.7307 - val_loss: 1.1887 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.15836\n",
      "Epoch 147/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9008 - accuracy: 0.7467 - val_loss: 1.1794 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.15836\n",
      "Epoch 148/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9175 - accuracy: 0.7333 - val_loss: 1.1871 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.15836\n",
      "Epoch 149/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8801 - accuracy: 0.7333 - val_loss: 1.1785 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.15836\n",
      "Epoch 150/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8761 - accuracy: 0.7440 - val_loss: 1.1878 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.15836\n",
      "Epoch 151/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9041 - accuracy: 0.7387 - val_loss: 1.1803 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.15836\n",
      "Epoch 152/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9252 - accuracy: 0.7120 - val_loss: 1.1841 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.15836\n",
      "Epoch 153/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8846 - accuracy: 0.7360 - val_loss: 1.1834 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.15836\n",
      "Epoch 154/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8857 - accuracy: 0.7307 - val_loss: 1.1832 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.15836\n",
      "Epoch 155/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9185 - accuracy: 0.7227 - val_loss: 1.1954 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.15836\n",
      "Epoch 156/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9437 - accuracy: 0.7147 - val_loss: 1.1771 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.15836\n",
      "Epoch 157/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9049 - accuracy: 0.7360 - val_loss: 1.1771 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.15836\n",
      "Epoch 158/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8761 - accuracy: 0.7413 - val_loss: 1.1805 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.15836\n",
      "Epoch 159/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8963 - accuracy: 0.7280 - val_loss: 1.1934 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.15836\n",
      "Epoch 160/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9190 - accuracy: 0.7280 - val_loss: 1.1772 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.15836\n",
      "Epoch 161/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9317 - accuracy: 0.7253 - val_loss: 1.1874 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.15836\n",
      "Epoch 162/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9006 - accuracy: 0.7253 - val_loss: 1.1878 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.15836\n",
      "Epoch 163/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9118 - accuracy: 0.7120 - val_loss: 1.1845 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.15836\n",
      "Epoch 164/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8878 - accuracy: 0.7307 - val_loss: 1.1947 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.15836\n",
      "Epoch 165/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8641 - accuracy: 0.7467 - val_loss: 1.1875 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.15836\n",
      "Epoch 166/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8893 - accuracy: 0.7307 - val_loss: 1.1928 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.15836\n",
      "Epoch 167/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8927 - accuracy: 0.7227 - val_loss: 1.1899 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.15836\n",
      "Epoch 168/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8697 - accuracy: 0.7547 - val_loss: 1.1803 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.15836\n",
      "Epoch 169/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8841 - accuracy: 0.7387 - val_loss: 1.1960 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.15836\n",
      "Epoch 170/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9014 - accuracy: 0.7387 - val_loss: 1.1990 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.15836\n",
      "Epoch 171/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8790 - accuracy: 0.7653 - val_loss: 1.1865 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.15836\n",
      "Epoch 172/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8774 - accuracy: 0.7467 - val_loss: 1.1885 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.15836\n",
      "Epoch 173/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8978 - accuracy: 0.7200 - val_loss: 1.1849 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.15836\n",
      "Epoch 174/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8932 - accuracy: 0.7307 - val_loss: 1.1778 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.15836\n",
      "Epoch 175/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8722 - accuracy: 0.7333 - val_loss: 1.1969 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.15836\n",
      "Epoch 176/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8868 - accuracy: 0.7333 - val_loss: 1.1851 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.15836\n",
      "Epoch 177/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8697 - accuracy: 0.7360 - val_loss: 1.2005 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.15836\n",
      "Epoch 178/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8688 - accuracy: 0.7440 - val_loss: 1.1940 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.15836\n",
      "Epoch 179/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9042 - accuracy: 0.7173 - val_loss: 1.2094 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.15836\n",
      "Epoch 180/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8457 - accuracy: 0.7573 - val_loss: 1.1875 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.15836\n",
      "Epoch 181/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8716 - accuracy: 0.7387 - val_loss: 1.1915 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.15836\n",
      "Epoch 182/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8834 - accuracy: 0.7333 - val_loss: 1.1976 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.15836\n",
      "Epoch 183/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8649 - accuracy: 0.7387 - val_loss: 1.1762 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.15836\n",
      "Epoch 184/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8580 - accuracy: 0.7387 - val_loss: 1.1906 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.15836\n",
      "Epoch 185/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8978 - accuracy: 0.7307 - val_loss: 1.1934 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.15836\n",
      "Epoch 186/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8916 - accuracy: 0.7440 - val_loss: 1.1849 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.15836\n",
      "Epoch 187/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8948 - accuracy: 0.7200 - val_loss: 1.1887 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.15836\n",
      "Epoch 188/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8699 - accuracy: 0.7413 - val_loss: 1.1765 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.15836\n",
      "Epoch 189/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.9053 - accuracy: 0.7200 - val_loss: 1.1908 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.15836\n",
      "Epoch 190/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8935 - accuracy: 0.7387 - val_loss: 1.1831 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.15836\n",
      "Epoch 191/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8996 - accuracy: 0.7200 - val_loss: 1.1929 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.15836\n",
      "Epoch 192/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8800 - accuracy: 0.7387 - val_loss: 1.1744 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.15836\n",
      "Epoch 193/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8861 - accuracy: 0.7307 - val_loss: 1.1920 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.15836\n",
      "Epoch 194/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8746 - accuracy: 0.7440 - val_loss: 1.1899 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.15836\n",
      "Epoch 195/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8687 - accuracy: 0.7360 - val_loss: 1.1717 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.15836\n",
      "Epoch 196/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8597 - accuracy: 0.7547 - val_loss: 1.1782 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.15836\n",
      "Epoch 197/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.9099 - accuracy: 0.7253 - val_loss: 1.1797 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.15836\n",
      "Epoch 198/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8688 - accuracy: 0.7627 - val_loss: 1.1808 - val_accuracy: 0.6880\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.15836\n",
      "Epoch 199/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8653 - accuracy: 0.7467 - val_loss: 1.1949 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.15836\n",
      "Epoch 200/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8655 - accuracy: 0.7573 - val_loss: 1.1776 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.15836\n",
      "Epoch 201/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8686 - accuracy: 0.7573 - val_loss: 1.1863 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.15836\n",
      "Epoch 202/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8576 - accuracy: 0.7467 - val_loss: 1.1953 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.15836\n",
      "Epoch 203/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8768 - accuracy: 0.7520 - val_loss: 1.1925 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.15836\n",
      "Epoch 204/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8882 - accuracy: 0.7253 - val_loss: 1.1935 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.15836\n",
      "Epoch 205/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8844 - accuracy: 0.7387 - val_loss: 1.1745 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.15836\n",
      "Epoch 206/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8935 - accuracy: 0.7467 - val_loss: 1.1813 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.15836\n",
      "Epoch 207/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8682 - accuracy: 0.7547 - val_loss: 1.1695 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.15836\n",
      "Epoch 208/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8858 - accuracy: 0.7467 - val_loss: 1.1895 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.15836\n",
      "Epoch 209/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8739 - accuracy: 0.7387 - val_loss: 1.1696 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.15836\n",
      "Epoch 210/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8729 - accuracy: 0.7333 - val_loss: 1.1770 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.15836\n",
      "Epoch 211/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8658 - accuracy: 0.7573 - val_loss: 1.2029 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.15836\n",
      "Epoch 212/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8551 - accuracy: 0.7493 - val_loss: 1.1830 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.15836\n",
      "Epoch 213/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8252 - accuracy: 0.7707 - val_loss: 1.1942 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.15836\n",
      "Epoch 214/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8683 - accuracy: 0.7493 - val_loss: 1.2050 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.15836\n",
      "Epoch 215/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8471 - accuracy: 0.7467 - val_loss: 1.1927 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.15836\n",
      "Epoch 216/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8773 - accuracy: 0.7413 - val_loss: 1.1956 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.15836\n",
      "Epoch 217/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8513 - accuracy: 0.7600 - val_loss: 1.2091 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.15836\n",
      "Epoch 218/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8771 - accuracy: 0.7360 - val_loss: 1.1963 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.15836\n",
      "Epoch 219/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8596 - accuracy: 0.7440 - val_loss: 1.2119 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.15836\n",
      "Epoch 220/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8563 - accuracy: 0.7413 - val_loss: 1.2055 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.15836\n",
      "Epoch 221/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8623 - accuracy: 0.7680 - val_loss: 1.2175 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.15836\n",
      "Epoch 222/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8459 - accuracy: 0.7653 - val_loss: 1.1850 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.15836\n",
      "Epoch 223/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8594 - accuracy: 0.7600 - val_loss: 1.2126 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.15836\n",
      "Epoch 224/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8592 - accuracy: 0.7413 - val_loss: 1.1989 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.15836\n",
      "Epoch 225/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8721 - accuracy: 0.7573 - val_loss: 1.1913 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.15836\n",
      "Epoch 226/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8281 - accuracy: 0.7680 - val_loss: 1.2079 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.15836\n",
      "Epoch 227/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8722 - accuracy: 0.7307 - val_loss: 1.2112 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.15836\n",
      "Epoch 228/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8521 - accuracy: 0.7573 - val_loss: 1.1950 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.15836\n",
      "Epoch 229/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8649 - accuracy: 0.7413 - val_loss: 1.1892 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.15836\n",
      "Epoch 230/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8706 - accuracy: 0.7307 - val_loss: 1.2031 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.15836\n",
      "Epoch 231/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8935 - accuracy: 0.7227 - val_loss: 1.2103 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.15836\n",
      "Epoch 232/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8567 - accuracy: 0.7467 - val_loss: 1.1889 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.15836\n",
      "Epoch 233/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8642 - accuracy: 0.7520 - val_loss: 1.2030 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.15836\n",
      "Epoch 234/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8858 - accuracy: 0.7360 - val_loss: 1.1813 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.15836\n",
      "Epoch 235/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8663 - accuracy: 0.7387 - val_loss: 1.2059 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.15836\n",
      "Epoch 236/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8777 - accuracy: 0.7413 - val_loss: 1.1870 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.15836\n",
      "Epoch 237/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8704 - accuracy: 0.7280 - val_loss: 1.2077 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.15836\n",
      "Epoch 238/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8860 - accuracy: 0.7173 - val_loss: 1.1840 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.15836\n",
      "Epoch 239/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8769 - accuracy: 0.7307 - val_loss: 1.2029 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.15836\n",
      "Epoch 240/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8688 - accuracy: 0.7333 - val_loss: 1.1861 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.15836\n",
      "Epoch 241/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8465 - accuracy: 0.7787 - val_loss: 1.2080 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.15836\n",
      "Epoch 242/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.9020 - accuracy: 0.7333 - val_loss: 1.1969 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.15836\n",
      "Epoch 243/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8347 - accuracy: 0.7547 - val_loss: 1.2065 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.15836\n",
      "Epoch 244/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8524 - accuracy: 0.7680 - val_loss: 1.2125 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.15836\n",
      "Epoch 245/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8367 - accuracy: 0.7733 - val_loss: 1.2054 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.15836\n",
      "Epoch 246/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8322 - accuracy: 0.7547 - val_loss: 1.2125 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.15836\n",
      "Epoch 247/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8469 - accuracy: 0.7653 - val_loss: 1.2226 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.15836\n",
      "Epoch 248/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8991 - accuracy: 0.7280 - val_loss: 1.1940 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.15836\n",
      "Epoch 249/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8364 - accuracy: 0.7680 - val_loss: 1.1885 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.15836\n",
      "Epoch 250/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8715 - accuracy: 0.7280 - val_loss: 1.2015 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.15836\n",
      "Epoch 251/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8624 - accuracy: 0.7520 - val_loss: 1.1935 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.15836\n",
      "Epoch 252/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8634 - accuracy: 0.7520 - val_loss: 1.2061 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.15836\n",
      "Epoch 253/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8455 - accuracy: 0.7627 - val_loss: 1.2020 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.15836\n",
      "Epoch 254/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8737 - accuracy: 0.7547 - val_loss: 1.2061 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.15836\n",
      "Epoch 255/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8456 - accuracy: 0.7493 - val_loss: 1.1991 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.15836\n",
      "Epoch 256/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8334 - accuracy: 0.7600 - val_loss: 1.2193 - val_accuracy: 0.6880\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.15836\n",
      "Epoch 257/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8349 - accuracy: 0.7493 - val_loss: 1.2155 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.15836\n",
      "Epoch 258/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8424 - accuracy: 0.7573 - val_loss: 1.2026 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.15836\n",
      "Epoch 259/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8249 - accuracy: 0.7547 - val_loss: 1.2092 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.15836\n",
      "Epoch 260/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8436 - accuracy: 0.7547 - val_loss: 1.2164 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.15836\n",
      "Epoch 261/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8600 - accuracy: 0.7680 - val_loss: 1.2140 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.15836\n",
      "Epoch 262/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8754 - accuracy: 0.7280 - val_loss: 1.2008 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.15836\n",
      "Epoch 263/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8458 - accuracy: 0.7627 - val_loss: 1.1980 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.15836\n",
      "Epoch 264/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8328 - accuracy: 0.7413 - val_loss: 1.1949 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.15836\n",
      "Epoch 265/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8215 - accuracy: 0.7600 - val_loss: 1.1932 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.15836\n",
      "Epoch 266/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8366 - accuracy: 0.7787 - val_loss: 1.2012 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.15836\n",
      "Epoch 267/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8624 - accuracy: 0.7440 - val_loss: 1.2057 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.15836\n",
      "Epoch 268/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8529 - accuracy: 0.7387 - val_loss: 1.1943 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.15836\n",
      "Epoch 269/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8222 - accuracy: 0.7733 - val_loss: 1.1960 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.15836\n",
      "Epoch 270/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8496 - accuracy: 0.7600 - val_loss: 1.1962 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.15836\n",
      "Epoch 271/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8468 - accuracy: 0.7680 - val_loss: 1.1924 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.15836\n",
      "Epoch 272/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8131 - accuracy: 0.7707 - val_loss: 1.1852 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.15836\n",
      "Epoch 273/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8224 - accuracy: 0.7600 - val_loss: 1.1981 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.15836\n",
      "Epoch 274/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8410 - accuracy: 0.7413 - val_loss: 1.2019 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.15836\n",
      "Epoch 275/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8909 - accuracy: 0.7227 - val_loss: 1.1929 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.15836\n",
      "Epoch 276/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8666 - accuracy: 0.7333 - val_loss: 1.1955 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.15836\n",
      "Epoch 277/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8580 - accuracy: 0.7707 - val_loss: 1.1990 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.15836\n",
      "Epoch 278/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8328 - accuracy: 0.7680 - val_loss: 1.2240 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.15836\n",
      "Epoch 279/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8368 - accuracy: 0.7600 - val_loss: 1.1957 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.15836\n",
      "Epoch 280/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8548 - accuracy: 0.7547 - val_loss: 1.2043 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.15836\n",
      "Epoch 281/600\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.8589 - accuracy: 0.7387 - val_loss: 1.1887 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.15836\n",
      "Epoch 282/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8293 - accuracy: 0.7653 - val_loss: 1.2284 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.15836\n",
      "Epoch 283/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8255 - accuracy: 0.7707 - val_loss: 1.1998 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.15836\n",
      "Epoch 284/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8430 - accuracy: 0.7493 - val_loss: 1.1933 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 1.15836\n",
      "Epoch 285/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8301 - accuracy: 0.7413 - val_loss: 1.2039 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.15836\n",
      "Epoch 286/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8448 - accuracy: 0.7467 - val_loss: 1.2174 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.15836\n",
      "Epoch 287/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8481 - accuracy: 0.7547 - val_loss: 1.1999 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.15836\n",
      "Epoch 288/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8888 - accuracy: 0.7013 - val_loss: 1.2154 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.15836\n",
      "Epoch 289/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7985 - accuracy: 0.7653 - val_loss: 1.2308 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.15836\n",
      "Epoch 290/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8277 - accuracy: 0.7653 - val_loss: 1.2193 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.15836\n",
      "Epoch 291/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8330 - accuracy: 0.7680 - val_loss: 1.2132 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.15836\n",
      "Epoch 292/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8140 - accuracy: 0.7680 - val_loss: 1.2215 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.15836\n",
      "Epoch 293/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8482 - accuracy: 0.7547 - val_loss: 1.2209 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.15836\n",
      "Epoch 294/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8015 - accuracy: 0.7627 - val_loss: 1.2183 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 1.15836\n",
      "Epoch 295/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8396 - accuracy: 0.7467 - val_loss: 1.2157 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.15836\n",
      "Epoch 296/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8199 - accuracy: 0.7547 - val_loss: 1.2103 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 1.15836\n",
      "Epoch 297/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8681 - accuracy: 0.7360 - val_loss: 1.2289 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.15836\n",
      "Epoch 298/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8460 - accuracy: 0.7360 - val_loss: 1.2310 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.15836\n",
      "Epoch 299/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8503 - accuracy: 0.7440 - val_loss: 1.2072 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.15836\n",
      "Epoch 300/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8236 - accuracy: 0.7520 - val_loss: 1.2073 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.15836\n",
      "Epoch 301/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8224 - accuracy: 0.7573 - val_loss: 1.2098 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 1.15836\n",
      "Epoch 302/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8341 - accuracy: 0.7547 - val_loss: 1.2147 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 1.15836\n",
      "Epoch 303/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8261 - accuracy: 0.7813 - val_loss: 1.2258 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 1.15836\n",
      "Epoch 304/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8084 - accuracy: 0.8000 - val_loss: 1.2217 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 1.15836\n",
      "Epoch 305/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8101 - accuracy: 0.7573 - val_loss: 1.2254 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 1.15836\n",
      "Epoch 306/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8265 - accuracy: 0.7653 - val_loss: 1.2070 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 1.15836\n",
      "Epoch 307/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8447 - accuracy: 0.7173 - val_loss: 1.2178 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 1.15836\n",
      "Epoch 308/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8574 - accuracy: 0.7627 - val_loss: 1.2022 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 1.15836\n",
      "Epoch 309/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8519 - accuracy: 0.7547 - val_loss: 1.2205 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 1.15836\n",
      "Epoch 310/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8308 - accuracy: 0.7707 - val_loss: 1.2125 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 1.15836\n",
      "Epoch 311/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8429 - accuracy: 0.7573 - val_loss: 1.2091 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 1.15836\n",
      "Epoch 312/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8631 - accuracy: 0.7387 - val_loss: 1.2233 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 1.15836\n",
      "Epoch 313/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8355 - accuracy: 0.7653 - val_loss: 1.2029 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 1.15836\n",
      "Epoch 314/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8138 - accuracy: 0.7760 - val_loss: 1.2107 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 1.15836\n",
      "Epoch 315/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8314 - accuracy: 0.7520 - val_loss: 1.2079 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 1.15836\n",
      "Epoch 316/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8344 - accuracy: 0.7680 - val_loss: 1.2269 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 1.15836\n",
      "Epoch 317/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8001 - accuracy: 0.7547 - val_loss: 1.2210 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 1.15836\n",
      "Epoch 318/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8333 - accuracy: 0.7440 - val_loss: 1.2039 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 1.15836\n",
      "Epoch 319/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8219 - accuracy: 0.7627 - val_loss: 1.2004 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 1.15836\n",
      "Epoch 320/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8383 - accuracy: 0.7787 - val_loss: 1.2130 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 1.15836\n",
      "Epoch 321/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8014 - accuracy: 0.7707 - val_loss: 1.2118 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 1.15836\n",
      "Epoch 322/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8150 - accuracy: 0.7733 - val_loss: 1.2238 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 1.15836\n",
      "Epoch 323/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8106 - accuracy: 0.7467 - val_loss: 1.2047 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 1.15836\n",
      "Epoch 324/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8307 - accuracy: 0.7387 - val_loss: 1.2282 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 1.15836\n",
      "Epoch 325/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8438 - accuracy: 0.7467 - val_loss: 1.2219 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 1.15836\n",
      "Epoch 326/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8293 - accuracy: 0.7493 - val_loss: 1.2183 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 1.15836\n",
      "Epoch 327/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8128 - accuracy: 0.7707 - val_loss: 1.2018 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 1.15836\n",
      "Epoch 328/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8084 - accuracy: 0.7947 - val_loss: 1.2077 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 1.15836\n",
      "Epoch 329/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8203 - accuracy: 0.7680 - val_loss: 1.2189 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 1.15836\n",
      "Epoch 330/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7938 - accuracy: 0.7760 - val_loss: 1.2060 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 1.15836\n",
      "Epoch 331/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8599 - accuracy: 0.7387 - val_loss: 1.2164 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 1.15836\n",
      "Epoch 332/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8137 - accuracy: 0.7333 - val_loss: 1.2228 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 1.15836\n",
      "Epoch 333/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8225 - accuracy: 0.7520 - val_loss: 1.2137 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 1.15836\n",
      "Epoch 334/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8024 - accuracy: 0.7413 - val_loss: 1.2116 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 1.15836\n",
      "Epoch 335/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8105 - accuracy: 0.7653 - val_loss: 1.2155 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 1.15836\n",
      "Epoch 336/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8279 - accuracy: 0.7707 - val_loss: 1.2089 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 1.15836\n",
      "Epoch 337/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8329 - accuracy: 0.7440 - val_loss: 1.2361 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 1.15836\n",
      "Epoch 338/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8199 - accuracy: 0.7547 - val_loss: 1.2135 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 1.15836\n",
      "Epoch 339/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8207 - accuracy: 0.7493 - val_loss: 1.2307 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 1.15836\n",
      "Epoch 340/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8176 - accuracy: 0.7760 - val_loss: 1.2390 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 1.15836\n",
      "Epoch 341/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8053 - accuracy: 0.7787 - val_loss: 1.2172 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 1.15836\n",
      "Epoch 342/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7996 - accuracy: 0.7893 - val_loss: 1.2141 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 1.15836\n",
      "Epoch 343/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8242 - accuracy: 0.7600 - val_loss: 1.2062 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 1.15836\n",
      "Epoch 344/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8243 - accuracy: 0.7627 - val_loss: 1.2139 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 1.15836\n",
      "Epoch 345/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8190 - accuracy: 0.7573 - val_loss: 1.1990 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 1.15836\n",
      "Epoch 346/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8187 - accuracy: 0.7520 - val_loss: 1.2253 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 1.15836\n",
      "Epoch 347/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8210 - accuracy: 0.7760 - val_loss: 1.2173 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 1.15836\n",
      "Epoch 348/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8357 - accuracy: 0.7440 - val_loss: 1.2230 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 1.15836\n",
      "Epoch 349/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8133 - accuracy: 0.7680 - val_loss: 1.2105 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 1.15836\n",
      "Epoch 350/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8194 - accuracy: 0.7627 - val_loss: 1.2058 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 1.15836\n",
      "Epoch 351/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8053 - accuracy: 0.7920 - val_loss: 1.2402 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 1.15836\n",
      "Epoch 352/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8214 - accuracy: 0.7627 - val_loss: 1.1960 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 1.15836\n",
      "Epoch 353/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8081 - accuracy: 0.7653 - val_loss: 1.2032 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 1.15836\n",
      "Epoch 354/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8026 - accuracy: 0.7760 - val_loss: 1.2131 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 1.15836\n",
      "Epoch 355/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8131 - accuracy: 0.7600 - val_loss: 1.2211 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 1.15836\n",
      "Epoch 356/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8293 - accuracy: 0.7493 - val_loss: 1.2064 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 1.15836\n",
      "Epoch 357/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8362 - accuracy: 0.7707 - val_loss: 1.2193 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 1.15836\n",
      "Epoch 358/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8201 - accuracy: 0.7600 - val_loss: 1.2120 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 1.15836\n",
      "Epoch 359/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8075 - accuracy: 0.7840 - val_loss: 1.1967 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 1.15836\n",
      "Epoch 360/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8028 - accuracy: 0.7840 - val_loss: 1.2204 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 1.15836\n",
      "Epoch 361/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8146 - accuracy: 0.7600 - val_loss: 1.2145 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 1.15836\n",
      "Epoch 362/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8083 - accuracy: 0.7680 - val_loss: 1.2223 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 1.15836\n",
      "Epoch 363/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7899 - accuracy: 0.7813 - val_loss: 1.2017 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 1.15836\n",
      "Epoch 364/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8002 - accuracy: 0.7813 - val_loss: 1.2139 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 1.15836\n",
      "Epoch 365/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8159 - accuracy: 0.7547 - val_loss: 1.2344 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 1.15836\n",
      "Epoch 366/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7880 - accuracy: 0.7893 - val_loss: 1.2263 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 1.15836\n",
      "Epoch 367/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7962 - accuracy: 0.7813 - val_loss: 1.2320 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 1.15836\n",
      "Epoch 368/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8157 - accuracy: 0.7707 - val_loss: 1.2192 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 1.15836\n",
      "Epoch 369/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8408 - accuracy: 0.7387 - val_loss: 1.2226 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 1.15836\n",
      "Epoch 370/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7941 - accuracy: 0.7787 - val_loss: 1.2051 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 1.15836\n",
      "Epoch 371/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8084 - accuracy: 0.7707 - val_loss: 1.2044 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 1.15836\n",
      "Epoch 372/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8218 - accuracy: 0.7573 - val_loss: 1.2240 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 1.15836\n",
      "Epoch 373/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8084 - accuracy: 0.7707 - val_loss: 1.2299 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 1.15836\n",
      "Epoch 374/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8132 - accuracy: 0.7520 - val_loss: 1.2252 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 1.15836\n",
      "Epoch 375/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8161 - accuracy: 0.7787 - val_loss: 1.2239 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 1.15836\n",
      "Epoch 376/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7889 - accuracy: 0.7733 - val_loss: 1.2138 - val_accuracy: 0.6080\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 1.15836\n",
      "Epoch 377/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7750 - accuracy: 0.7840 - val_loss: 1.2234 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 1.15836\n",
      "Epoch 378/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8020 - accuracy: 0.7707 - val_loss: 1.2283 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 1.15836\n",
      "Epoch 379/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8040 - accuracy: 0.7787 - val_loss: 1.2381 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 1.15836\n",
      "Epoch 380/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8138 - accuracy: 0.7413 - val_loss: 1.2510 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 1.15836\n",
      "Epoch 381/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8280 - accuracy: 0.7493 - val_loss: 1.2511 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 1.15836\n",
      "Epoch 382/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7727 - accuracy: 0.7867 - val_loss: 1.2433 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 1.15836\n",
      "Epoch 383/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7959 - accuracy: 0.7840 - val_loss: 1.2502 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 1.15836\n",
      "Epoch 384/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7619 - accuracy: 0.7787 - val_loss: 1.2283 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 1.15836\n",
      "Epoch 385/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8233 - accuracy: 0.7547 - val_loss: 1.2687 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 1.15836\n",
      "Epoch 386/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8423 - accuracy: 0.7573 - val_loss: 1.2554 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 1.15836\n",
      "Epoch 387/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8206 - accuracy: 0.7467 - val_loss: 1.2222 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 1.15836\n",
      "Epoch 388/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7583 - accuracy: 0.7947 - val_loss: 1.2149 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 1.15836\n",
      "Epoch 389/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7892 - accuracy: 0.7947 - val_loss: 1.2200 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 1.15836\n",
      "Epoch 390/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7980 - accuracy: 0.7840 - val_loss: 1.2131 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 1.15836\n",
      "Epoch 391/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7827 - accuracy: 0.7760 - val_loss: 1.2243 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 1.15836\n",
      "Epoch 392/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8131 - accuracy: 0.7787 - val_loss: 1.2351 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 1.15836\n",
      "Epoch 393/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8060 - accuracy: 0.7520 - val_loss: 1.2195 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 1.15836\n",
      "Epoch 394/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7507 - accuracy: 0.8027 - val_loss: 1.2090 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 1.15836\n",
      "Epoch 395/600\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.7929 - accuracy: 0.7707 - val_loss: 1.2119 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 1.15836\n",
      "Epoch 396/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.8159 - accuracy: 0.7440 - val_loss: 1.2031 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 1.15836\n",
      "Epoch 397/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.7960 - accuracy: 0.7840 - val_loss: 1.2196 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 1.15836\n",
      "Epoch 398/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.8062 - accuracy: 0.7627 - val_loss: 1.2136 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 1.15836\n",
      "Epoch 399/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8289 - accuracy: 0.7760 - val_loss: 1.2026 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 1.15836\n",
      "Epoch 400/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8118 - accuracy: 0.7947 - val_loss: 1.2277 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 1.15836\n",
      "Epoch 401/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7757 - accuracy: 0.7760 - val_loss: 1.2126 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 1.15836\n",
      "Epoch 402/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8021 - accuracy: 0.7600 - val_loss: 1.2349 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 1.15836\n",
      "Epoch 403/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8168 - accuracy: 0.7973 - val_loss: 1.2060 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 1.15836\n",
      "Epoch 404/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7944 - accuracy: 0.7813 - val_loss: 1.2363 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 1.15836\n",
      "Epoch 405/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7632 - accuracy: 0.7840 - val_loss: 1.2270 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 1.15836\n",
      "Epoch 406/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7814 - accuracy: 0.7680 - val_loss: 1.2287 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 1.15836\n",
      "Epoch 407/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7745 - accuracy: 0.7840 - val_loss: 1.2379 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 1.15836\n",
      "Epoch 408/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7749 - accuracy: 0.7867 - val_loss: 1.2299 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 1.15836\n",
      "Epoch 409/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7806 - accuracy: 0.7680 - val_loss: 1.2277 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 1.15836\n",
      "Epoch 410/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7802 - accuracy: 0.8053 - val_loss: 1.2417 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 1.15836\n",
      "Epoch 411/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7878 - accuracy: 0.7813 - val_loss: 1.2558 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 1.15836\n",
      "Epoch 412/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7841 - accuracy: 0.7760 - val_loss: 1.2261 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 1.15836\n",
      "Epoch 413/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7939 - accuracy: 0.7813 - val_loss: 1.2170 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 1.15836\n",
      "Epoch 414/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7744 - accuracy: 0.7920 - val_loss: 1.2315 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 1.15836\n",
      "Epoch 415/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7786 - accuracy: 0.7573 - val_loss: 1.2240 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 1.15836\n",
      "Epoch 416/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7893 - accuracy: 0.7733 - val_loss: 1.2330 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 1.15836\n",
      "Epoch 417/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7471 - accuracy: 0.7760 - val_loss: 1.2499 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 1.15836\n",
      "Epoch 418/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7823 - accuracy: 0.7707 - val_loss: 1.2505 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 1.15836\n",
      "Epoch 419/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7807 - accuracy: 0.7893 - val_loss: 1.2481 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 1.15836\n",
      "Epoch 420/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7706 - accuracy: 0.7733 - val_loss: 1.2415 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 1.15836\n",
      "Epoch 421/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7730 - accuracy: 0.7893 - val_loss: 1.2399 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 1.15836\n",
      "Epoch 422/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7935 - accuracy: 0.7947 - val_loss: 1.2338 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 1.15836\n",
      "Epoch 423/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7747 - accuracy: 0.7653 - val_loss: 1.2213 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 1.15836\n",
      "Epoch 424/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7818 - accuracy: 0.7760 - val_loss: 1.2421 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 1.15836\n",
      "Epoch 425/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7777 - accuracy: 0.7867 - val_loss: 1.2350 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 1.15836\n",
      "Epoch 426/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7757 - accuracy: 0.7707 - val_loss: 1.2474 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 1.15836\n",
      "Epoch 427/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8085 - accuracy: 0.7627 - val_loss: 1.2501 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 1.15836\n",
      "Epoch 428/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7909 - accuracy: 0.7733 - val_loss: 1.2504 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 1.15836\n",
      "Epoch 429/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7622 - accuracy: 0.7893 - val_loss: 1.2197 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 1.15836\n",
      "Epoch 430/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7747 - accuracy: 0.7867 - val_loss: 1.2404 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 1.15836\n",
      "Epoch 431/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7848 - accuracy: 0.7840 - val_loss: 1.2561 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 1.15836\n",
      "Epoch 432/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7986 - accuracy: 0.7653 - val_loss: 1.2433 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 1.15836\n",
      "Epoch 433/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7588 - accuracy: 0.7813 - val_loss: 1.2504 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 1.15836\n",
      "Epoch 434/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7988 - accuracy: 0.7707 - val_loss: 1.2475 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 1.15836\n",
      "Epoch 435/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7755 - accuracy: 0.7947 - val_loss: 1.2427 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 1.15836\n",
      "Epoch 436/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7782 - accuracy: 0.7840 - val_loss: 1.2484 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 1.15836\n",
      "Epoch 437/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7540 - accuracy: 0.8027 - val_loss: 1.2527 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 1.15836\n",
      "Epoch 438/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7932 - accuracy: 0.7653 - val_loss: 1.2372 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 1.15836\n",
      "Epoch 439/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7888 - accuracy: 0.7653 - val_loss: 1.2649 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 1.15836\n",
      "Epoch 440/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8158 - accuracy: 0.7680 - val_loss: 1.2585 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 1.15836\n",
      "Epoch 441/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7477 - accuracy: 0.7920 - val_loss: 1.2473 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 1.15836\n",
      "Epoch 442/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7704 - accuracy: 0.7653 - val_loss: 1.2302 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 1.15836\n",
      "Epoch 443/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.8021 - accuracy: 0.7680 - val_loss: 1.2336 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 1.15836\n",
      "Epoch 444/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7632 - accuracy: 0.7653 - val_loss: 1.2667 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 1.15836\n",
      "Epoch 445/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.7899 - accuracy: 0.7760 - val_loss: 1.2416 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 1.15836\n",
      "Epoch 446/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.8163 - accuracy: 0.7440 - val_loss: 1.2543 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 1.15836\n",
      "Epoch 447/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7854 - accuracy: 0.7813 - val_loss: 1.2556 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 1.15836\n",
      "Epoch 448/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7955 - accuracy: 0.7760 - val_loss: 1.2225 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 1.15836\n",
      "Epoch 449/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7516 - accuracy: 0.8027 - val_loss: 1.2495 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 1.15836\n",
      "Epoch 450/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7590 - accuracy: 0.7760 - val_loss: 1.2543 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 1.15836\n",
      "Epoch 451/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7709 - accuracy: 0.7867 - val_loss: 1.2638 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 1.15836\n",
      "Epoch 452/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7515 - accuracy: 0.7920 - val_loss: 1.2422 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 1.15836\n",
      "Epoch 453/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7710 - accuracy: 0.7840 - val_loss: 1.2482 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 1.15836\n",
      "Epoch 454/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7822 - accuracy: 0.7867 - val_loss: 1.2375 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 1.15836\n",
      "Epoch 455/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7261 - accuracy: 0.8053 - val_loss: 1.2574 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 1.15836\n",
      "Epoch 456/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7587 - accuracy: 0.7707 - val_loss: 1.2516 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 1.15836\n",
      "Epoch 457/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7486 - accuracy: 0.8000 - val_loss: 1.2573 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 1.15836\n",
      "Epoch 458/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7953 - accuracy: 0.7867 - val_loss: 1.2643 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 1.15836\n",
      "Epoch 459/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7654 - accuracy: 0.7680 - val_loss: 1.2529 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 1.15836\n",
      "Epoch 460/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7803 - accuracy: 0.7973 - val_loss: 1.2640 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 1.15836\n",
      "Epoch 461/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7649 - accuracy: 0.8053 - val_loss: 1.2754 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 1.15836\n",
      "Epoch 462/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7558 - accuracy: 0.7947 - val_loss: 1.2679 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 1.15836\n",
      "Epoch 463/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7978 - accuracy: 0.7893 - val_loss: 1.2617 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 1.15836\n",
      "Epoch 464/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7426 - accuracy: 0.7867 - val_loss: 1.2494 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 1.15836\n",
      "Epoch 465/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7619 - accuracy: 0.8000 - val_loss: 1.2756 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 1.15836\n",
      "Epoch 466/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7459 - accuracy: 0.8027 - val_loss: 1.2613 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 1.15836\n",
      "Epoch 467/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8031 - accuracy: 0.7680 - val_loss: 1.2678 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 1.15836\n",
      "Epoch 468/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7848 - accuracy: 0.7867 - val_loss: 1.2550 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 1.15836\n",
      "Epoch 469/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7598 - accuracy: 0.8080 - val_loss: 1.2669 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 1.15836\n",
      "Epoch 470/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7458 - accuracy: 0.8027 - val_loss: 1.2485 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 1.15836\n",
      "Epoch 471/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7584 - accuracy: 0.7867 - val_loss: 1.2656 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 1.15836\n",
      "Epoch 472/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7649 - accuracy: 0.7867 - val_loss: 1.2472 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 1.15836\n",
      "Epoch 473/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7327 - accuracy: 0.7893 - val_loss: 1.2741 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 1.15836\n",
      "Epoch 474/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7283 - accuracy: 0.7893 - val_loss: 1.2778 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 1.15836\n",
      "Epoch 475/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7392 - accuracy: 0.7973 - val_loss: 1.2593 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 1.15836\n",
      "Epoch 476/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7612 - accuracy: 0.7947 - val_loss: 1.2601 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 1.15836\n",
      "Epoch 477/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7773 - accuracy: 0.7867 - val_loss: 1.2795 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 1.15836\n",
      "Epoch 478/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7385 - accuracy: 0.7947 - val_loss: 1.2721 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 1.15836\n",
      "Epoch 479/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7533 - accuracy: 0.7920 - val_loss: 1.2775 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 1.15836\n",
      "Epoch 480/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7523 - accuracy: 0.7707 - val_loss: 1.2726 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 1.15836\n",
      "Epoch 481/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7442 - accuracy: 0.7973 - val_loss: 1.2686 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 1.15836\n",
      "Epoch 482/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7525 - accuracy: 0.8053 - val_loss: 1.2724 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 1.15836\n",
      "Epoch 483/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7600 - accuracy: 0.7947 - val_loss: 1.2927 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 1.15836\n",
      "Epoch 484/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7768 - accuracy: 0.7947 - val_loss: 1.2616 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 1.15836\n",
      "Epoch 485/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7694 - accuracy: 0.7733 - val_loss: 1.2448 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 1.15836\n",
      "Epoch 486/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7556 - accuracy: 0.7893 - val_loss: 1.2769 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 1.15836\n",
      "Epoch 487/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7694 - accuracy: 0.7813 - val_loss: 1.2802 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 1.15836\n",
      "Epoch 488/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7878 - accuracy: 0.7947 - val_loss: 1.2614 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 1.15836\n",
      "Epoch 489/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7278 - accuracy: 0.7947 - val_loss: 1.2742 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 1.15836\n",
      "Epoch 490/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7314 - accuracy: 0.8160 - val_loss: 1.2627 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 1.15836\n",
      "Epoch 491/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7584 - accuracy: 0.7947 - val_loss: 1.2699 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 1.15836\n",
      "Epoch 492/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7117 - accuracy: 0.8267 - val_loss: 1.2841 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 1.15836\n",
      "Epoch 493/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7690 - accuracy: 0.7840 - val_loss: 1.2790 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 1.15836\n",
      "Epoch 494/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7623 - accuracy: 0.8000 - val_loss: 1.2914 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 1.15836\n",
      "Epoch 495/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7854 - accuracy: 0.7813 - val_loss: 1.3054 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 1.15836\n",
      "Epoch 496/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7429 - accuracy: 0.8160 - val_loss: 1.2791 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 1.15836\n",
      "Epoch 497/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7625 - accuracy: 0.7813 - val_loss: 1.2727 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 1.15836\n",
      "Epoch 498/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7215 - accuracy: 0.8133 - val_loss: 1.2771 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 1.15836\n",
      "Epoch 499/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7672 - accuracy: 0.7893 - val_loss: 1.2881 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 1.15836\n",
      "Epoch 500/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7427 - accuracy: 0.7893 - val_loss: 1.3005 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 1.15836\n",
      "Epoch 501/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7039 - accuracy: 0.8240 - val_loss: 1.2775 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 1.15836\n",
      "Epoch 502/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7329 - accuracy: 0.8053 - val_loss: 1.2895 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 1.15836\n",
      "Epoch 503/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7485 - accuracy: 0.7947 - val_loss: 1.2834 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 1.15836\n",
      "Epoch 504/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7541 - accuracy: 0.8000 - val_loss: 1.2698 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 1.15836\n",
      "Epoch 505/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7503 - accuracy: 0.7867 - val_loss: 1.2800 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 1.15836\n",
      "Epoch 506/600\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.7391 - accuracy: 0.7867 - val_loss: 1.2723 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 1.15836\n",
      "Epoch 507/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.7419 - accuracy: 0.7920 - val_loss: 1.2817 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 1.15836\n",
      "Epoch 508/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7384 - accuracy: 0.8133 - val_loss: 1.2874 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 1.15836\n",
      "Epoch 509/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7165 - accuracy: 0.8133 - val_loss: 1.2769 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 1.15836\n",
      "Epoch 510/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7617 - accuracy: 0.7787 - val_loss: 1.3052 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 1.15836\n",
      "Epoch 511/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7422 - accuracy: 0.7920 - val_loss: 1.3041 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 1.15836\n",
      "Epoch 512/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7444 - accuracy: 0.8267 - val_loss: 1.2777 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 1.15836\n",
      "Epoch 513/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7186 - accuracy: 0.8160 - val_loss: 1.2700 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 1.15836\n",
      "Epoch 514/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7550 - accuracy: 0.8027 - val_loss: 1.2836 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 1.15836\n",
      "Epoch 515/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7477 - accuracy: 0.8027 - val_loss: 1.2798 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 1.15836\n",
      "Epoch 516/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.7401 - accuracy: 0.8080 - val_loss: 1.2930 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 1.15836\n",
      "Epoch 517/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7331 - accuracy: 0.8240 - val_loss: 1.2758 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 1.15836\n",
      "Epoch 518/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7255 - accuracy: 0.8213 - val_loss: 1.2985 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 1.15836\n",
      "Epoch 519/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7229 - accuracy: 0.8187 - val_loss: 1.2849 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 1.15836\n",
      "Epoch 520/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7338 - accuracy: 0.8053 - val_loss: 1.3030 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 1.15836\n",
      "Epoch 521/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7251 - accuracy: 0.8027 - val_loss: 1.3286 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 1.15836\n",
      "Epoch 522/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.82 - 0s 16ms/step - loss: 0.7246 - accuracy: 0.8133 - val_loss: 1.3002 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 1.15836\n",
      "Epoch 523/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7017 - accuracy: 0.8053 - val_loss: 1.2999 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 1.15836\n",
      "Epoch 524/600\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.7278 - accuracy: 0.8027 - val_loss: 1.2937 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 1.15836\n",
      "Epoch 525/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7354 - accuracy: 0.8053 - val_loss: 1.2744 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 1.15836\n",
      "Epoch 526/600\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.7097 - accuracy: 0.8240 - val_loss: 1.2621 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 1.15836\n",
      "Epoch 527/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7293 - accuracy: 0.8080 - val_loss: 1.2824 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 1.15836\n",
      "Epoch 528/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7030 - accuracy: 0.8160 - val_loss: 1.2758 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 1.15836\n",
      "Epoch 529/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7468 - accuracy: 0.7947 - val_loss: 1.3050 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 1.15836\n",
      "Epoch 530/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7320 - accuracy: 0.8107 - val_loss: 1.3056 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 1.15836\n",
      "Epoch 531/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7291 - accuracy: 0.7973 - val_loss: 1.3334 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 1.15836\n",
      "Epoch 532/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7242 - accuracy: 0.8160 - val_loss: 1.2888 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 1.15836\n",
      "Epoch 533/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7614 - accuracy: 0.7760 - val_loss: 1.3228 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 1.15836\n",
      "Epoch 534/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7401 - accuracy: 0.7893 - val_loss: 1.3180 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 1.15836\n",
      "Epoch 535/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7393 - accuracy: 0.8053 - val_loss: 1.2976 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 1.15836\n",
      "Epoch 536/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7434 - accuracy: 0.8000 - val_loss: 1.3076 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 1.15836\n",
      "Epoch 537/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7570 - accuracy: 0.7947 - val_loss: 1.3008 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 1.15836\n",
      "Epoch 538/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7341 - accuracy: 0.8107 - val_loss: 1.3057 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 1.15836\n",
      "Epoch 539/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7231 - accuracy: 0.8133 - val_loss: 1.2959 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 1.15836\n",
      "Epoch 540/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7390 - accuracy: 0.8053 - val_loss: 1.3154 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 1.15836\n",
      "Epoch 541/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7521 - accuracy: 0.8053 - val_loss: 1.2833 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 1.15836\n",
      "Epoch 542/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7691 - accuracy: 0.8080 - val_loss: 1.3044 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 1.15836\n",
      "Epoch 543/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7669 - accuracy: 0.8000 - val_loss: 1.2955 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 1.15836\n",
      "Epoch 544/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7117 - accuracy: 0.8240 - val_loss: 1.3144 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 1.15836\n",
      "Epoch 545/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7099 - accuracy: 0.8053 - val_loss: 1.2776 - val_accuracy: 0.6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00545: val_loss did not improve from 1.15836\n",
      "Epoch 546/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7295 - accuracy: 0.7947 - val_loss: 1.2990 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 1.15836\n",
      "Epoch 547/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7035 - accuracy: 0.8347 - val_loss: 1.2966 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 1.15836\n",
      "Epoch 548/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7338 - accuracy: 0.8133 - val_loss: 1.3006 - val_accuracy: 0.6080\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 1.15836\n",
      "Epoch 549/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7121 - accuracy: 0.8267 - val_loss: 1.3061 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 1.15836\n",
      "Epoch 550/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7169 - accuracy: 0.8080 - val_loss: 1.2966 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 1.15836\n",
      "Epoch 551/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7262 - accuracy: 0.8160 - val_loss: 1.2945 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 1.15836\n",
      "Epoch 552/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7331 - accuracy: 0.8187 - val_loss: 1.3027 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 1.15836\n",
      "Epoch 553/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7235 - accuracy: 0.7947 - val_loss: 1.3104 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 1.15836\n",
      "Epoch 554/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7060 - accuracy: 0.8160 - val_loss: 1.3345 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 1.15836\n",
      "Epoch 555/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7174 - accuracy: 0.8080 - val_loss: 1.3049 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 1.15836\n",
      "Epoch 556/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.6972 - accuracy: 0.8213 - val_loss: 1.3059 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 1.15836\n",
      "Epoch 557/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7269 - accuracy: 0.8000 - val_loss: 1.3393 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 1.15836\n",
      "Epoch 558/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.6869 - accuracy: 0.8453 - val_loss: 1.3247 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 1.15836\n",
      "Epoch 559/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7274 - accuracy: 0.8133 - val_loss: 1.3384 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 1.15836\n",
      "Epoch 560/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7245 - accuracy: 0.8293 - val_loss: 1.3129 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 1.15836\n",
      "Epoch 561/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.6969 - accuracy: 0.8240 - val_loss: 1.3074 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 1.15836\n",
      "Epoch 562/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.6791 - accuracy: 0.8347 - val_loss: 1.3200 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 1.15836\n",
      "Epoch 563/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7123 - accuracy: 0.8160 - val_loss: 1.3272 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 1.15836\n",
      "Epoch 564/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7581 - accuracy: 0.8000 - val_loss: 1.3126 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 1.15836\n",
      "Epoch 565/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7168 - accuracy: 0.8373 - val_loss: 1.3086 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 1.15836\n",
      "Epoch 566/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7174 - accuracy: 0.8160 - val_loss: 1.3385 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 1.15836\n",
      "Epoch 567/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7353 - accuracy: 0.8107 - val_loss: 1.3308 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 1.15836\n",
      "Epoch 568/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.7062 - accuracy: 0.8267 - val_loss: 1.2998 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 1.15836\n",
      "Epoch 569/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7076 - accuracy: 0.8080 - val_loss: 1.3265 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 1.15836\n",
      "Epoch 570/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.6957 - accuracy: 0.8187 - val_loss: 1.3047 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 1.15836\n",
      "Epoch 571/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7005 - accuracy: 0.8240 - val_loss: 1.3044 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 1.15836\n",
      "Epoch 572/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7144 - accuracy: 0.8187 - val_loss: 1.3392 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 1.15836\n",
      "Epoch 573/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.6883 - accuracy: 0.8213 - val_loss: 1.3049 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 1.15836\n",
      "Epoch 574/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7302 - accuracy: 0.7867 - val_loss: 1.3000 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 1.15836\n",
      "Epoch 575/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7101 - accuracy: 0.8133 - val_loss: 1.3155 - val_accuracy: 0.6640\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 1.15836\n",
      "Epoch 576/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7315 - accuracy: 0.8107 - val_loss: 1.3041 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 1.15836\n",
      "Epoch 577/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.6908 - accuracy: 0.8373 - val_loss: 1.3232 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 1.15836\n",
      "Epoch 578/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7093 - accuracy: 0.8240 - val_loss: 1.3354 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 1.15836\n",
      "Epoch 579/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7488 - accuracy: 0.8267 - val_loss: 1.3241 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 1.15836\n",
      "Epoch 580/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7082 - accuracy: 0.8267 - val_loss: 1.3115 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 1.15836\n",
      "Epoch 581/600\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7390 - accuracy: 0.8000 - val_loss: 1.3074 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 1.15836\n",
      "Epoch 582/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7472 - accuracy: 0.8027 - val_loss: 1.3133 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 1.15836\n",
      "Epoch 583/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7101 - accuracy: 0.8267 - val_loss: 1.3062 - val_accuracy: 0.6080\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 1.15836\n",
      "Epoch 584/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.6885 - accuracy: 0.8427 - val_loss: 1.3278 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 1.15836\n",
      "Epoch 585/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7167 - accuracy: 0.8293 - val_loss: 1.3145 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 1.15836\n",
      "Epoch 586/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.7345 - accuracy: 0.8267 - val_loss: 1.3236 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 1.15836\n",
      "Epoch 587/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.6965 - accuracy: 0.8267 - val_loss: 1.3341 - val_accuracy: 0.6480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00587: val_loss did not improve from 1.15836\n",
      "Epoch 588/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.6935 - accuracy: 0.8240 - val_loss: 1.3203 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 1.15836\n",
      "Epoch 589/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.6903 - accuracy: 0.8160 - val_loss: 1.3180 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 1.15836\n",
      "Epoch 590/600\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.6829 - accuracy: 0.8347 - val_loss: 1.3392 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 1.15836\n",
      "Epoch 591/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7127 - accuracy: 0.8240 - val_loss: 1.3101 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 1.15836\n",
      "Epoch 592/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7308 - accuracy: 0.7973 - val_loss: 1.3243 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 1.15836\n",
      "Epoch 593/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.6794 - accuracy: 0.8213 - val_loss: 1.3196 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 1.15836\n",
      "Epoch 594/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7186 - accuracy: 0.8000 - val_loss: 1.3662 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 1.15836\n",
      "Epoch 595/600\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.6958 - accuracy: 0.8240 - val_loss: 1.3484 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 1.15836\n",
      "Epoch 596/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.6954 - accuracy: 0.8160 - val_loss: 1.3247 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 1.15836\n",
      "Epoch 597/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.6665 - accuracy: 0.8453 - val_loss: 1.3864 - val_accuracy: 0.6240\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 1.15836\n",
      "Epoch 598/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7238 - accuracy: 0.8027 - val_loss: 1.3456 - val_accuracy: 0.6160\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 1.15836\n",
      "Epoch 599/600\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.7161 - accuracy: 0.8160 - val_loss: 1.3500 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 1.15836\n",
      "Epoch 600/600\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.6970 - accuracy: 0.8107 - val_loss: 1.3572 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 1.15836\n",
      "Time taken: 0:01:44.480626\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\n",
    "from datetime import datetime\n",
    "# early_stopping= EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5) \n",
    "check_pointer = ModelCheckpoint(filepath = 'clf-resnet-checkpoint.hdf5',verbose=1,save_best_only=True) \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',mode='min',verbose=1,patience=5,min_delta = 0.0001,factor=0.2) \n",
    "callbacks = [check_pointer,early_stopping,reduce_lr]\n",
    "\n",
    "startTime = datetime.now()\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    hist = model.fit(XScaled, y_train,\n",
    "                    validation_data = (TestScaled,y_test),\n",
    "                    epochs = 600,\n",
    "                    batch_size = 32\n",
    "                    ,callbacks = [check_pointer]\n",
    "#                      ,early_stopping\n",
    "                        )\n",
    "print(\"Time taken:\", datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa3ac9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "fig, axs = plt.subplots(2)\n",
    "\n",
    "# accuracy \n",
    "axs[0].plot(hist.history[\"accuracy\"], label=\"train accuracy\")\n",
    "axs[0].plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")    \n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].legend(loc=\"lower right\")\n",
    "axs[0].set_title(\"Accuracy eval\")\n",
    "    \n",
    "# Error \n",
    "axs[1].plot(hist.history[\"loss\"], label=\"train error\")\n",
    "axs[1].plot(hist.history[\"val_loss\"], label=\"test error\")    \n",
    "axs[1].set_ylabel(\"Error\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].legend(loc=\"upper right\")\n",
    "axs[1].set_title(\"Error eval\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0e93a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3572 - accuracy: 0.6400\n",
      "The test loss is  1.357243537902832\n",
      "The best accuracy is:  63.999998569488525\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc=model.evaluate(TestScaled,y_test,batch_size=128)\n",
    "print(\"The test loss is \",test_loss)\n",
    "print(\"The best accuracy is: \",test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3024f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1061cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f81bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
